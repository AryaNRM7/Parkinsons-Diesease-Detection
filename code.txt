import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
#import seaborn as sns
#sns.set()

#%matplotlib inline


dis = pd.read_csv('C:/Users/khang/Desktop/dm/parkinsons.csv')
#print(dis) 
print(dis.shape)
tnull = dis.isnull().sum()
#tobsv = dis.shape(0)
#null_per = tnull[tnull>0]/tobsv*100
#tnull =  tnull[tnull>0] 
print(tnull) 
dupli = dis[dis.duplicated(keep = "first")].index 
print(dupli) 
dis.drop(dupli,axis=0,inplace=True)
dis[dis.duplicated(keep="first")]
print(dis.shape) 
fig,ax = plt.subplots(figsize=(11,11))
print(dis.hist(bins=50,ax=ax)) 
#Lets first drop the name column as it has no impact on the status
print(dis) 
dis.drop(columns ='Name',inplace=True)
X = dis.drop('Status', 1)
y = dis['Status']

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
cols = X.columns
print(cols)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
# Let us scale training data set test data using MinMaxScaler
scaler = MinMaxScaler()
train_scaled = scaler.fit_transform(X_train)
test_scaled = scaler.fit_transform(X_test)

X_train_scaled = pd.DataFrame(train_scaled)
X_test_scaled = pd.DataFrame(test_scaled)

# Replace the Columns Headers back
X_train_scaled.columns = cols
X_test_scaled.columns = cols
X_train_scaled.head()
#from sklearn.decomposition import PCA
#pca = PCA()
#X_train = pca.fit_transform(X_train)
#X_test = pca.transform(X_test)
#explained_variance = pca.explained_variance_ratio_
#print(explained_variance)
numRows = X_train_scaled.count(axis=0)[0]
print("Total no. of rows in the training set: ", numRows)
numRows = X_test_scaled.count(axis=0)[0]
print("Total no. of rows in the testing set: ", numRows)
#KNN
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.metrics import confusion_matrix,f1_score,accuracy_score, classification_report
from sklearn.preprocessing import MinMaxScaler
from sklearn.neighbors import KNeighborsClassifier

 
knn = KNeighborsClassifier(n_neighbors = 1)
 
knn.fit(X_train, y_train)
pred = knn.predict(X_test)
 
# Predictions and Evaluations
# Let's evaluate our KNN model !
from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(y_test, pred))
 
print(classification_report(y_test, pred))
error_rate = []
 
# Will take some time
for i in range(1, 40):
     
    knn = KNeighborsClassifier(n_neighbors = i)
    knn.fit(X_train, y_train)
    pred_i = knn.predict(X_test)
    error_rate.append(np.mean(pred_i != y_test))
 
plt.figure(figsize =(10, 6))
plt.plot(range(1, 40), error_rate, color ='blue',
                linestyle ='dashed', marker ='o',
         markerfacecolor ='red', markersize = 10)
 
plt.title('Error Rate vs. K Value')
plt.xlabel('K')
plt.ylabel('Error Rate')
knn = KNeighborsClassifier(n_neighbors = 1)
 
knn.fit(X_train, y_train)
pred = knn.predict(X_test)
 
print('WITH K = 1')
print('\n')
print(confusion_matrix(y_test, pred))
print('\n')
print(classification_report(y_test, pred))
 
 
# NOW WITH K = 15
knn = KNeighborsClassifier(n_neighbors = 15)
 
knn.fit(X_train, y_train)
pred = knn.predict(X_test)
 
print('WITH K = 15')
print('\n')
print(confusion_matrix(y_test, pred))
print('\n')
print(classification_report(y_test, pred))
K = 4

NNH = KNeighborsClassifier(n_neighbors= K)

# Call Nearest Neighbour algorithm
NNH.fit(X_train_scaled, y_train)
KNN_predicted_labels = NNH.predict(X_train_scaled)
train_acc = metrics.accuracy_score(y_train, KNN_predicted_labels)
print("Model Accuracy with Training Data: {0:.4f}".format(metrics.accuracy_score(y_train, KNN_predicted_labels)*100))
print()

KNN_predicted_labels = NNH.predict(X_test_scaled)
test_acc = metrics.accuracy_score(y_test, KNN_predicted_labels)
print("Model Accuracy with Testing Data: {0:.4f}".format(metrics.accuracy_score(y_test, KNN_predicted_labels)*100))
print()
# Gaussian Naive Bayes
from sklearn import datasets
from sklearn import metrics
from sklearn.naive_bayes import GaussianNB
GNB = GaussianNB()
GNB.fit(X_train_scaled, y_train)
GNB_predicted_labels = GNB.predict(X_train_scaled)

print("Model Accuracy with Training Data: {0:.4f}".format(metrics.accuracy_score(y_train, GNB_predicted_labels)*100))
print()

GNB_predicted_labels = GNB.predict(X_test_scaled)

print("Model Accuracy with Testing data: {0:.4f}".format(metrics.accuracy_score(y_test, GNB_predicted_labels)*100))
print()
#DECISION_TREE
# Function to perform training with giniIndex.
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
def train_using_gini(X_train, X_test, y_train):
  
    # Creating the classifier object
    clf_gini = DecisionTreeClassifier(criterion = "gini",
            random_state = 100,max_depth=3, min_samples_leaf=5)
  
    # Performing training
    clf_gini.fit(X_train, y_train)
    return clf_gini
      
# Function to perform training with entropy.
def tarin_using_entropy(X_train, X_test, y_train):
  
    # Decision tree with entropy
    clf_entropy = DecisionTreeClassifier(
            criterion = "entropy", random_state = 100,
            max_depth = 3, min_samples_leaf = 5)
  
    # Performing training
    clf_entropy.fit(X_train, y_train)
    return clf_entropy
  
  
# Function to make predictions
def prediction(X_test, clf_object):
  
    # Predicton on test with giniIndex
    y_pred = clf_object.predict(X_test)
    print("Predicted values:")
    print(y_pred)
    return y_pred
      
# Function to calculate accuracy
def cal_accuracy(y_test, y_pred):
      
    print("Confusion Matrix: ",
        confusion_matrix(y_test, y_pred))
      
    print ("Accuracy : ",
    accuracy_score(y_test,y_pred)*100)
      
    print("Report : ",
    classification_report(y_test, y_pred))
  
# Driver code
def main():
      
    # Building Phase
    clf_gini = train_using_gini(X_train, X_test, y_train)
    clf_entropy = tarin_using_entropy(X_train, X_test, y_train)
      
    # Operational Phase
    print("Results Using Gini Index:")
      
    # Prediction using gini
    y_pred_gini = prediction(X_test, clf_gini)
    cal_accuracy(y_test, y_pred_gini)
      
    print("Results Using Entropy:")
    # Prediction using entropy
    y_pred_entropy = prediction(X_test, clf_entropy)
    cal_accuracy(y_test, y_pred_entropy)
      
      
# Calling main function
if __name__=="__main__":
    main()


